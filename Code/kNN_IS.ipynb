{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuKCH-T2jp0R",
        "outputId": "2b343b21-f8a3-4129-87a4-b32cb00d3750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d0e_IFtswTB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import heapq\n",
        "from heapq import heapify, heappush, heappop\n",
        "import collections\n",
        "import csv\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD_Uf6OFjrnP"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().setAppName(\"word count\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDCMEvQnjuN2",
        "outputId": "cdc3eb93-6be5-4857-9a56-fd938049a23d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVzr0TH2B2kX"
      },
      "outputs": [],
      "source": [
        "train_ds = '/content/drive/MyDrive/Big Data Processing Labs/Project/train_data_500.csv'\n",
        "test_ds =  '/content/drive/MyDrive/Big Data Processing Labs/Project/test_set_250.csv'\n",
        "\n",
        "# train_ds = '/content/drive/MyDrive/traindata_medium.csv'\n",
        "# test_ds =  '/content/drive/MyDrive/test_set_medium.csv'\n",
        "\n",
        "#  different size of datasets that we have used for the experiments :\n",
        "\n",
        "# train_ds = '/content/drive/MyDrive/datasets/Datasets/train_set.csv'\n",
        "# test_ds =  '/content/drive/MyDrive/datasets/Datasets/test_set_small.csv'\n",
        "# test_ds = '/content/drive/MyDrive/datasets/Datasets/test_set_medium.csv'\n",
        "# test_ds = '/content/drive/MyDrive/datasets/Datasets/test_set_large1.csv'\n",
        "# test_ds = '/content/drive/MyDrive/datasets/Datasets/test_set_large2.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BScBuYCLjwkW",
        "outputId": "dd742e0e-84bc-42bd-b65c-3d93d339a4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.0, 13.0, 4.0, 13.0, 4.0, 13.0, 4.0, 13.0, 4.0, 13.0, 6.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n",
            "[4.0, 13.0, 4.0, 13.0, 4.0, 13.0, 4.0, 13.0, 4.0, 13.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "#  for the normalization : find maximum and minimum values for each feature\n",
        "mx_val_tr = []\n",
        "mn_val_tr = []\n",
        "mx_val_ts = []\n",
        "mn_val_ts = []\n",
        "numTrainSamples = 0\n",
        "numTestSamples = 0\n",
        "\n",
        "# opening the CSV file\n",
        "with open(train_ds, mode ='r')as file:\n",
        "\n",
        "  # reading the CSV file\n",
        "  csvFile = csv.reader(file)\n",
        "  enter = False\n",
        "\n",
        "  # displaying the contents of the CSV file\n",
        "  for row_list in csvFile:\n",
        "      # only digit row\n",
        "      if(row_list[0][0].isdigit()):\n",
        "        numTrainSamples += 1  # calculating no of rows\n",
        "        for i in range(0,len(row_list)): # iterating over rows\n",
        "          if(enter==False):\n",
        "            # first digit row\n",
        "            mx_val_tr.append(float(row_list[i]))\n",
        "            mn_val_tr.append(float(row_list[i]))\n",
        "          else:\n",
        "            # to find minimum and maximum\n",
        "            mx_val_tr[i] = max(mx_val_tr[i],float(row_list[i])) # updating maximum value for ith column\n",
        "            mn_val_tr[i] = min(mn_val_tr[i],float(row_list[i])) # updating minimum value for ith column\n",
        "        enter = True\n",
        "\n",
        "# opening the CSV file\n",
        "with open(test_ds, mode ='r')as file:\n",
        "\n",
        "  # reading the CSV file\n",
        "  csvFile = csv.reader(file)\n",
        "  enter = False\n",
        "\n",
        "  # displaying the contents of the CSV file\n",
        "  for row_list in csvFile:\n",
        "    if(row_list[0].isdigit()):\n",
        "      numTestSamples += 1 # calculating no of rows\n",
        "      for i in range(1,len(row_list)): # iterating over rows\n",
        "        if(enter==False):\n",
        "          mx_val_ts.append(float(row_list[i]))\n",
        "          mn_val_ts.append(float(row_list[i]))\n",
        "        else:\n",
        "          mx_val_ts[i-1] = max(mx_val_ts[i-1],float(row_list[i])) # updating maximum value for ith column\n",
        "          mn_val_ts[i-1] = min(mn_val_ts[i-1],float(row_list[i])) # updating minimum value for ith column\n",
        "      enter = True\n",
        "\n",
        "#  get maximum and minimum for each column of the datasets\n",
        "print(mx_val_tr)\n",
        "print(mn_val_tr)\n",
        "print(mx_val_ts)\n",
        "print(mn_val_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAZKT0i4jz1u"
      },
      "outputs": [],
      "source": [
        "#  normalization function for the training and test dataset\n",
        "#  using the formula : (curr_val[i] -min(ith column))/(max(ith column) - min(ith column))\n",
        "\n",
        "def normalize_tr(row,mn_val_tr,mx_val_tr):\n",
        "    row_list = row.split(\",\")\n",
        "    list_ans = []\n",
        "    if(row_list[0][0].isdigit()):\n",
        "      list_ans.append(int(row_list[-1])) # put last entry as hand for the simplicity\n",
        "      for i in range(0,len(row_list)): # iterating over each row and normalizing it\n",
        "        list_ans.append((float(row_list[i]) - mn_val_tr[i])/(mx_val_tr[i]-mn_val_tr[i]))\n",
        "    return list_ans\n",
        "\n",
        "def normalize_ts(row,mn_val_ts,mx_val_ts):\n",
        "    row_list = row.split(\",\")\n",
        "    list_ans = []\n",
        "    if(row_list[0][0].isdigit()):\n",
        "      list_ans.append(int(row_list[0])) # put first entry as test id for the simplicity\n",
        "      for i in range(1,len(row_list)): # iterating over each row and normalizing it\n",
        "        list_ans.append((float(row_list[i]) - mn_val_ts[i-1])/(mx_val_ts[i-1]-mn_val_ts[i-1]))\n",
        "    return list_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZuwRHh7j7Ec"
      },
      "outputs": [],
      "source": [
        "# different number of mappers\n",
        "# number_map = 10\n",
        "number_map = 50\n",
        "# number_map = 100\n",
        "# number_map = 200\n",
        "\n",
        "numFeatures = 10 # number of columns used to find euclidean distance\n",
        "\n",
        "tr_rdd_raw = sc.textFile(train_ds,number_map)\n",
        "ts_rdd_raw = sc.textFile(test_ds)\n",
        "\n",
        "#  normalizing the both dataset\n",
        "tr_rdd = tr_rdd_raw.map(lambda line : normalize_tr(line,mn_val_tr,mx_val_tr)).cache()\n",
        "ts_rdd = ts_rdd_raw.map(lambda line : normalize_ts(line,mn_val_ts,mx_val_ts)).cache()\n",
        "\n",
        "# print(ts_rdd.take(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDQCWap4pZOd"
      },
      "outputs": [],
      "source": [
        "#  prefine function taken from github repository\n",
        "def calIter(numTrainSmps,numTestSmps,memAllow):\n",
        "  numIterations = 0;\n",
        "  weightTrain = (8 * numTrainSmps * numFeatures) / (number_map * 1024.0 * 1024.0)\n",
        "  weightTest = (8 * numTestSmps * numFeatures) / (1024.0 * 1024.0)\n",
        "  # print(weightTrain,\" \",weightTest)\n",
        "  if (weightTrain + weightTest < memAllow * 1024.0):\n",
        "        numIterations = 1\n",
        "  else:\n",
        "    if (weightTrain >= memAllow * 1024.0):\n",
        "      print(\"Train wight bigger than lim-task. Abort\")\n",
        "      sys.exit(1)\n",
        "\n",
        "    numIterations = int((1 + (weightTest / ((memAllow * 1024.0) - weightTrain))))\n",
        "  return numIterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3qkBexY3L9p",
        "outputId": "39f0b301-fc6b-4236-c028-7a7d3543fe1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapPartitionsRDD[232] at mapPartitions at PythonRDD.scala:160"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "avail_mem = 0.2\n",
        "num_of_iter = calIter(numTrainSamples,numTestSamples,avail_mem) # get number  of iterations\n",
        "ts_rdd.partitionBy(num_of_iter,partitionFunc=range) # divide test dataset into num_of_iter parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuXY3lqligbj"
      },
      "outputs": [],
      "source": [
        "def computekNN(row,all_ts,k):\n",
        "  info = []\n",
        "  if(len(row) > 0):\n",
        "    point1 = np.array((row[1],row[2],row[3],row[4],row[5],row[6],row[7],row[8],row[9],row[10])) # row of training dataset\n",
        "    for i in range(0,len(all_ts)):\n",
        "      if(len(all_ts[i])>0):\n",
        "        point2 = np.array((all_ts[i][1],all_ts[i][2],all_ts[i][3],all_ts[i][4],all_ts[i][5],all_ts[i][6],all_ts[i][7],all_ts[i][8],all_ts[i][9],all_ts[i][10])) #row of test dataset\n",
        "        dist = np.linalg.norm(point1 - point2) # euclidean distance\n",
        "\n",
        "        info.append((all_ts[i][0],(dist,row[0]))) # return as list ((test_id,(distance,class)))\n",
        "  return info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2-0C7WHtAX5"
      },
      "outputs": [],
      "source": [
        "def CombineResults(row):\n",
        "  #  input row : (key = test_id,value = [(distance1,class1),(distance2,class2),...,(distancen,classn)])\n",
        "  info = []\n",
        "  max_heap = [] # initialize heap to find top k\n",
        "  heapq._heapify_max(max_heap)\n",
        "  for i in row[1]:\n",
        "    heappush(max_heap,(i[0],i[1]))\n",
        "    if (len(max_heap) > k): # pop if exceed k size\n",
        "      heapq._heappop_max(max_heap)\n",
        "\n",
        "  count = collections.defaultdict(lambda: 0) # initialize dictionary to count voting for each class in top k\n",
        "  ans = -1\n",
        "  curr_max_count = 0\n",
        "  for val in max_heap:\n",
        "    count[val[1]]+=1 # update count for each class\n",
        "    if(count[val[1]] > curr_max_count): # update answer if get more mejority voting\n",
        "      curr_max_count = count[val[1]]\n",
        "      ans = val[1]\n",
        "  return (row[0],ans) # (test_id,predicted_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATPMrhgogYxp",
        "outputId": "a6b46645-d957-4d8c-c15d-57ed77865b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(50, 0), (100, 0), (150, 0), (200, 0), (250, 0), (1, 0), (51, 0), (101, 0), (151, 0), (201, 0), (2, 0), (52, 0), (102, 0), (152, 0), (202, 0), (3, 0), (53, 0), (103, 0), (153, 0), (203, 0), (4, 0), (54, 0), (104, 0), (154, 0), (204, 0), (5, 0), (55, 0), (105, 0), (155, 0), (205, 0), (6, 0), (56, 0), (106, 0), (156, 0), (206, 0), (7, 0), (57, 0), (107, 0), (157, 0), (207, 0), (8, 0), (58, 0), (108, 0), (158, 0), (208, 0), (9, 0), (59, 0), (109, 0), (159, 0), (209, 0), (10, 0), (60, 0), (110, 1), (160, 0), (210, 0), (11, 0), (61, 0), (111, 0), (161, 0), (211, 0), (12, 0), (62, 0), (112, 0), (162, 1), (212, 0), (13, 0), (63, 0), (113, 0), (163, 0), (213, 0), (14, 0), (64, 0), (114, 0), (164, 0), (214, 0), (15, 1), (65, 0), (115, 0), (165, 0), (215, 1), (16, 0), (66, 0), (116, 0), (166, 0), (216, 0), (17, 0), (67, 0), (117, 1), (167, 0), (217, 0), (18, 0), (68, 0), (118, 0), (168, 0), (218, 0), (19, 0), (69, 0), (119, 0), (169, 0), (219, 0), (20, 0), (70, 0), (120, 0), (170, 0), (220, 0), (21, 0), (71, 0), (121, 0), (171, 0), (221, 0), (22, 0), (72, 0), (122, 0), (172, 0), (222, 0), (23, 0), (73, 0), (123, 0), (173, 0), (223, 0), (24, 0), (74, 0), (124, 1), (174, 0), (224, 0), (25, 0), (75, 0), (125, 0), (175, 0), (225, 0), (26, 1), (76, 0), (126, 0), (176, 0), (226, 0), (27, 0), (77, 0), (127, 0), (177, 0), (227, 0), (28, 0), (78, 0), (128, 0), (178, 0), (228, 0), (29, 0), (79, 0), (129, 0), (179, 0), (229, 0), (30, 0), (80, 0), (130, 0), (180, 0), (230, 0), (31, 0), (81, 0), (131, 0), (181, 0), (231, 0), (32, 0), (82, 0), (132, 0), (182, 0), (232, 0), (33, 0), (83, 0), (133, 0), (183, 0), (233, 0), (34, 0), (84, 0), (134, 0), (184, 0), (234, 0), (35, 0), (85, 0), (135, 0), (185, 0), (235, 0), (36, 0), (86, 0), (136, 0), (186, 0), (236, 0), (37, 0), (87, 0), (137, 0), (187, 0), (237, 0), (38, 0), (88, 0), (138, 0), (188, 0), (238, 1), (39, 0), (89, 0), (139, 0), (189, 0), (239, 0), (40, 0), (90, 0), (140, 0), (190, 0), (240, 0), (41, 0), (91, 0), (141, 0), (191, 0), (241, 0), (42, 0), (92, 0), (142, 0), (192, 0), (242, 0), (43, 0), (93, 0), (143, 0), (193, 0), (243, 0), (44, 0), (94, 0), (144, 0), (194, 0), (244, 0), (45, 0), (95, 0), (145, 0), (195, 0), (245, 0), (46, 0), (96, 0), (146, 0), (196, 0), (246, 0), (47, 0), (97, 0), (147, 0), (197, 0), (247, 0), (48, 0), (98, 0), (148, 0), (198, 0), (248, 0), (49, 0), (99, 0), (149, 0), (199, 0), (249, 0)]\n"
          ]
        }
      ],
      "source": [
        "broad_cast_data = ts_rdd.glom().collect() # collect list of distributed object\n",
        "k = 50 # set k values\n",
        "for ts_data in broad_cast_data:\n",
        "  resultkNN = tr_rdd.flatMap(lambda line : computekNN(line,ts_data,k))  # flatmap return -> list of((test_id,(distance,class)))\n",
        "  res = resultkNN.groupByKey().mapValues(list)  # grouping it by key return -> list of((key = test_id,value = list of((distance,class))))\n",
        "  get_ans = res.map(lambda line : CombineResults(line)) # return -> list of((test_id,predicted class))\n",
        "  print(get_ans.take(1000))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}